{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a33d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 1: CHECK GPU + HARDWARE SPECIFICATION\n",
    "# ======================================================\n",
    "import torch\n",
    "import platform\n",
    "import psutil\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU AVAILABILITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HARDWARE SPECIFICATIONS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"OS: {platform.system()} {platform.release()}\")\n",
    "print(f\"CPU: {platform.processor()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.2f} GB\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"GPU: Not available (using CPU)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 2: IMPORT LIBRARIES\n",
    "# ======================================================\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed untuk reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"\\n‚úÖ Libraries imported\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dde899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 3: CONFIGURATION\n",
    "# ======================================================\n",
    "# Path Configuration\n",
    "DATA_DIR = \".\"\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_CSV = os.path.join(DATA_DIR, \"test.csv\")\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_IMG_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "# Training Configuration\n",
    "CONFIG = {\n",
    "    'img_size': 224,\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_workers': 0,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 4: LOAD DATASET\n",
    "# ======================================================\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Clean data\n",
    "train_df = train_df.dropna(subset=['label']).reset_index(drop=True)\n",
    "test_df = test_df.dropna(subset=['label']).reset_index(drop=True)\n",
    "\n",
    "# Class mapping\n",
    "CLASS_NAMES = sorted(train_df['label'].unique())\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(CLASS_NAMES)}\n",
    "idx_to_class = {idx: cls for cls, idx in class_to_idx.items()}\n",
    "num_classes = len(CLASS_NAMES)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {CLASS_NAMES}\")\n",
    "\n",
    "# Train-validation split\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, test_size=0.2, random_state=42, stratify=train_df['label']\n",
    ")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {len(train_data)}\")\n",
    "print(f\"  Validation: {len(val_data)}\")\n",
    "print(f\"  Test: {len(test_df)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54eb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 5: DATA TRANSFORMS & DATASET CLASS\n",
    "# ======================================================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"\\n‚úÖ Data transforms defined\")\n",
    "\n",
    "# Dataset Class\n",
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['filename']\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.class_to_idx[self.df.iloc[idx]['label']]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d674b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 6: CREATE DATALOADERS\n",
    "# ======================================================\n",
    "train_dataset = FoodDataset(train_data, TRAIN_IMG_DIR, train_transform)\n",
    "val_dataset = FoodDataset(val_data, TRAIN_IMG_DIR, val_transform)\n",
    "test_dataset = FoodDataset(test_df, TEST_IMG_DIR, val_transform)\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets created:\")\n",
    "print(f\"  Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"  Val dataset: {len(val_dataset)} samples\")\n",
    "print(f\"  Test dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=True, \n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test one batch\n",
    "try:\n",
    "    test_iter = iter(test_loader)\n",
    "    batch_images, batch_labels = next(test_iter)\n",
    "    print(f\"\\n‚úÖ Test batch verification:\")\n",
    "    print(f\"  Batch images shape: {batch_images.shape}\")\n",
    "    print(f\"  Batch labels shape: {batch_labels.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading test batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 7: MODEL DEFINITIONS & UTILITY FUNCTIONS\n",
    "# ======================================================\n",
    "def get_vit_model(num_classes):\n",
    "    return timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=num_classes)\n",
    "\n",
    "def get_deit_model(num_classes):\n",
    "    return timm.create_model('deit_tiny_patch16_224', pretrained=True, num_classes=num_classes)\n",
    "\n",
    "def get_swin_model(num_classes):\n",
    "    return timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=num_classes)\n",
    "\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable = total - trainable\n",
    "    return total, trainable, non_trainable\n",
    "\n",
    "print(\"\\n‚úÖ Models defined: ViT, DeiT, Swin\")\n",
    "print(\"‚úÖ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 8: TRAINING FUNCTIONS\n",
    "# ======================================================\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc='Training', leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc='Validation', leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_name):\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRAINING: {model_name}\")\n",
    "    print('='*70)\n",
    "    print(f\"{'Epoch':<8} {'Train Loss':<12} {'Train Acc':<12} {'Val Loss':<12} {'Val Acc':<12} {'Status':<10}\")\n",
    "    print('-'*70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Status indicator\n",
    "        status = \"\"\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f\"best_{model_name}.pth\")\n",
    "            status = \"üíæ SAVED\"\n",
    "        \n",
    "        print(f\"{epoch+1:>3}/{num_epochs:<3} {train_loss:>11.4f} {train_acc:>10.2f}% {val_loss:>11.4f} {val_acc:>10.2f}% {status:<10}\")\n",
    "    \n",
    "    print('-'*70)\n",
    "    print(f\"‚úÖ Training completed! Best Val Acc: {best_val_acc:.2f}%\")\n",
    "    print('='*70)\n",
    "    return history\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 9: EVALUATION & INFERENCE FUNCTIONS\n",
    "# ======================================================\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(f\"   Debug: Dataloader has {len(dataloader)} batches\")\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            batch_count = 0\n",
    "            for images, labels in dataloader:\n",
    "                batch_count += 1\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy().tolist())\n",
    "                all_labels.extend(labels.cpu().numpy().tolist())\n",
    "            \n",
    "            print(f\"   Debug: Processed {batch_count} batches, collected {len(all_preds)} predictions\")\n",
    "        \n",
    "        if len(all_preds) == 0 or len(all_labels) == 0:\n",
    "            print(f\"   ‚ö†Ô∏è Warning: No predictions collected!\")\n",
    "            return {\n",
    "                'accuracy': 0.0,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'precision_per_class': np.zeros(num_classes),\n",
    "                'recall_per_class': np.zeros(num_classes),\n",
    "                'f1_per_class': np.zeros(num_classes),\n",
    "                'confusion_matrix': np.zeros((num_classes, num_classes))\n",
    "            }\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            all_labels, all_preds, average='weighted', zero_division=0\n",
    "        )\n",
    "        precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "            all_labels, all_preds, average=None, zero_division=0, labels=list(range(num_classes))\n",
    "        )\n",
    "        cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy * 100,\n",
    "            'precision': precision * 100,\n",
    "            'recall': recall * 100,\n",
    "            'f1_score': f1 * 100,\n",
    "            'precision_per_class': precision_per_class * 100,\n",
    "            'recall_per_class': recall_per_class * 100,\n",
    "            'f1_per_class': f1_per_class * 100,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'accuracy': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'precision_per_class': np.zeros(num_classes),\n",
    "            'recall_per_class': np.zeros(num_classes),\n",
    "            'f1_per_class': np.zeros(num_classes),\n",
    "            'confusion_matrix': np.zeros((num_classes, num_classes))\n",
    "        }\n",
    "\n",
    "\n",
    "def measure_inference_time(model, dataloader, device, warmup=10):\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    print(f\"   Debug: Dataloader has {len(dataloader)} batches\")\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            warmup_count = 0\n",
    "            for images, _ in dataloader:\n",
    "                if warmup_count >= warmup:\n",
    "                    break\n",
    "                images = images.to(device)\n",
    "                _ = model(images)\n",
    "                warmup_count += 1\n",
    "            \n",
    "            print(f\"   Debug: Warmup completed with {warmup_count} batches\")\n",
    "            \n",
    "            batch_count = 0\n",
    "            for images, _ in dataloader:\n",
    "                batch_count += 1\n",
    "                images = images.to(device)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                start_time = time.time()\n",
    "                _ = model(images)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                end_time = time.time()\n",
    "                times.append(end_time - start_time)\n",
    "            \n",
    "            print(f\"   Debug: Measured {batch_count} batches, collected {len(times)} times\")\n",
    "        \n",
    "        if len(times) == 0:\n",
    "            print(f\"   ‚ö†Ô∏è Warning: No inference times collected!\")\n",
    "            return {\n",
    "                'avg_per_image_ms': 0.0,\n",
    "                'std_per_image_ms': 0.0,\n",
    "                'total_test_time_s': 0.0,\n",
    "                'throughput_img_per_s': 0.0\n",
    "            }\n",
    "        \n",
    "        avg_batch_time = np.mean(times)\n",
    "        std_batch_time = np.std(times)\n",
    "        avg_per_image = avg_batch_time / CONFIG['batch_size']\n",
    "        std_per_image = std_batch_time / CONFIG['batch_size']\n",
    "        throughput = 1.0 / avg_per_image if avg_per_image > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'avg_per_image_ms': avg_per_image * 1000,\n",
    "            'std_per_image_ms': std_per_image * 1000,\n",
    "            'total_test_time_s': sum(times),\n",
    "            'throughput_img_per_s': throughput\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error during inference measurement: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'avg_per_image_ms': 0.0,\n",
    "            'std_per_image_ms': 0.0,\n",
    "            'total_test_time_s': 0.0,\n",
    "            'throughput_img_per_s': 0.0\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Evaluation and inference functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac03e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 10: TRAIN ALL MODELS\n",
    "# ======================================================\n",
    "device = CONFIG['device']\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "results = {}\n",
    "models_config = [\n",
    "    ('ViT_Tiny', get_vit_model),\n",
    "    ('DeiT_Tiny', get_deit_model),\n",
    "    ('Swin_Tiny', get_swin_model)\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING FOR ALL MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, model_fn in models_config:\n",
    "    print(f\"\\n{'‚ñà'*70}\")\n",
    "    print(f\"MODEL: {model_name}\")\n",
    "    print('‚ñà'*70)\n",
    "    \n",
    "    # Initialize\n",
    "    model = model_fn(num_classes).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], \n",
    "                           weight_decay=CONFIG['weight_decay'])\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params, trainable_params, non_trainable_params = count_parameters(model)\n",
    "    model_size_mb = total_params * 4 / (1024 ** 2)\n",
    "    \n",
    "    print(f\"\\nüìä Model Information:\")\n",
    "    print(f\"   Total Parameters    : {total_params:,}\")\n",
    "    print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"   Model Size          : {model_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Train\n",
    "    history = train_model(model, train_loader, val_loader, criterion, \n",
    "                         optimizer, CONFIG['num_epochs'], device, model_name)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\nüìà Evaluating on Test Set...\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(f\"best_{model_name}.pth\", map_location=device))\n",
    "        model.to(device)\n",
    "        eval_results = evaluate_model(model, test_loader, device)\n",
    "        \n",
    "        print(f\"\\nüéØ Test Results:\")\n",
    "        print(f\"   Accuracy : {eval_results['accuracy']:.2f}%\")\n",
    "        print(f\"   Precision: {eval_results['precision']:.2f}%\")\n",
    "        print(f\"   Recall   : {eval_results['recall']:.2f}%\")\n",
    "        print(f\"   F1-Score : {eval_results['f1_score']:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error loading or evaluating model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        eval_results = {\n",
    "            'accuracy': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'precision_per_class': np.zeros(num_classes),\n",
    "            'recall_per_class': np.zeros(num_classes),\n",
    "            'f1_per_class': np.zeros(num_classes),\n",
    "            'confusion_matrix': np.zeros((num_classes, num_classes))\n",
    "        }\n",
    "    \n",
    "    # Measure inference\n",
    "    print(f\"\\n‚ö° Measuring Inference Time...\")\n",
    "    inference_time = measure_inference_time(model, test_loader, device)\n",
    "    print(f\"   Per Image : {inference_time['avg_per_image_ms']:.2f} ¬± {inference_time['std_per_image_ms']:.2f} ms\")\n",
    "    print(f\"   Throughput: {inference_time['throughput_img_per_s']:.2f} images/sec\")\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'parameters': {\n",
    "            'total': total_params,\n",
    "            'trainable': trainable_params,\n",
    "            'non_trainable': non_trainable_params,\n",
    "            'size_mb': model_size_mb\n",
    "        },\n",
    "        'performance': eval_results,\n",
    "        'inference_time': inference_time,\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    del model, optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n‚úÖ {model_name} completed!\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ALL MODELS TRAINING COMPLETED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 11: COMPARISON TABLES\n",
    "# ======================================================\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Model':<15} {'Params(M)':<12} {'Size(MB)':<10} {'Acc(%)':<10} {'Prec(%)':<10} {'Rec(%)':<10} {'F1(%)':<10} {'Inf(ms)':<10}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    print(f\"{model_name:<15} \"\n",
    "          f\"{result['parameters']['total']/1e6:<12.2f} \"\n",
    "          f\"{result['parameters']['size_mb']:<10.2f} \"\n",
    "          f\"{result['performance']['accuracy']:<10.2f} \"\n",
    "          f\"{result['performance']['precision']:<10.2f} \"\n",
    "          f\"{result['performance']['recall']:<10.2f} \"\n",
    "          f\"{result['performance']['f1_score']:<10.2f} \"\n",
    "          f\"{result['inference_time']['avg_per_image_ms']:<10.2f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Per-class metrics table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS PERFORMANCE METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"{'Class':<20} {'Precision(%)':<15} {'Recall(%)':<15} {'F1-Score(%)':<15}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    prec_per_class = result['performance']['precision_per_class']\n",
    "    rec_per_class = result['performance']['recall_per_class']\n",
    "    f1_per_class = result['performance']['f1_per_class']\n",
    "    \n",
    "    if len(prec_per_class) == len(CLASS_NAMES):\n",
    "        for idx, class_name in enumerate(CLASS_NAMES):\n",
    "            print(f\"{class_name:<20} {prec_per_class[idx]:<15.2f} {rec_per_class[idx]:<15.2f} {f1_per_class[idx]:<15.2f}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: Per-class metrics length ({len(prec_per_class)}) doesn't match number of classes ({len(CLASS_NAMES)})\")\n",
    "        for idx in range(min(len(prec_per_class), len(CLASS_NAMES))):\n",
    "            class_name = CLASS_NAMES[idx] if idx < len(CLASS_NAMES) else f\"Class_{idx}\"\n",
    "            print(f\"{class_name:<20} {prec_per_class[idx]:<15.2f} {rec_per_class[idx]:<15.2f} {f1_per_class[idx]:<15.2f}\")\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'AVERAGE':<20} {result['performance']['precision']:<15.2f} {result['performance']['recall']:<15.2f} {result['performance']['f1_score']:<15.2f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 12: TRAINING CURVES VISUALIZATION\n",
    "# ======================================================\n",
    "print(\"\\nüìä Generating Training Curves...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss\n",
    "for model_name, result in results.items():\n",
    "    epochs = range(1, len(result['history']['train_loss']) + 1)\n",
    "    axes[0].plot(epochs, result['history']['train_loss'], '-o', label=f'{model_name} Train', linewidth=2, markersize=4)\n",
    "    axes[0].plot(epochs, result['history']['val_loss'], '--s', label=f'{model_name} Val', linewidth=2, markersize=4)\n",
    "\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy\n",
    "for model_name, result in results.items():\n",
    "    epochs = range(1, len(result['history']['train_acc']) + 1)\n",
    "    axes[1].plot(epochs, result['history']['train_acc'], '-o', label=f'{model_name} Train', linewidth=2, markersize=4)\n",
    "    axes[1].plot(epochs, result['history']['val_acc'], '--s', label=f'{model_name} Val', linewidth=2, markersize=4)\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training curves displayed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 13: CONFUSION MATRICES\n",
    "# ======================================================\n",
    "print(\"\\nüìä Generating Confusion Matrices...\")\n",
    "\n",
    "n_models = len(results)\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results.items()):\n",
    "    cm = result['performance']['confusion_matrix']\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    im = axes[idx].imshow(cm, cmap='Blues', aspect='auto', interpolation='nearest')\n",
    "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=12)\n",
    "    axes[idx].set_ylabel('Actual', fontsize=12)\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=axes[idx])\n",
    "    cbar.set_label('Count', fontsize=10)\n",
    "    \n",
    "    # Annotations\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            text_color = 'white' if cm[i, j] > cm.max()/2 else 'black'\n",
    "            axes[idx].text(j, i, int(cm[i, j]), \n",
    "                          ha=\"center\", va=\"center\", \n",
    "                          color=text_color, fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Ticks\n",
    "    axes[idx].set_xticks(range(len(CLASS_NAMES)))\n",
    "    axes[idx].set_yticks(range(len(CLASS_NAMES)))\n",
    "    axes[idx].set_xticklabels(CLASS_NAMES, rotation=45, ha='right', fontsize=10)\n",
    "    axes[idx].set_yticklabels(CLASS_NAMES, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Confusion matrices displayed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 14: PERFORMANCE METRICS COMPARISON\n",
    "# ======================================================\n",
    "print(\"\\nüìä Generating Performance Comparison...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "model_names = list(results.keys())\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "# Print data for verification\n",
    "print(\"Checking results data:\")\n",
    "for model_name in model_names:\n",
    "    print(f\"{model_name}:\")\n",
    "    for metric in metrics:\n",
    "        value = results[model_name]['performance'][metric]\n",
    "        print(f\"  {metric}: {value:.2f}\")\n",
    "\n",
    "# Plot bars\n",
    "for i, model_name in enumerate(model_names):\n",
    "    values = [results[model_name]['performance'][m] for m in metrics]\n",
    "    bars = ax.bar(x + i*width, values, width, label=model_name, color=colors[i], alpha=0.8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{height:.1f}',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Performance Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1-Score'], fontsize=11)\n",
    "ax.legend(fontsize=10, loc='lower right')\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 110])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Performance comparison displayed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50605b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 15: MODEL RECOMMENDATIONS\n",
    "# ======================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best accuracy\n",
    "best_acc_model = max(results.items(), key=lambda x: x[1]['performance']['accuracy'])\n",
    "print(f\"\\n‚úÖ BEST ACCURACY:\")\n",
    "print(f\"   Model: {best_acc_model[0]}\")\n",
    "print(f\"   Accuracy: {best_acc_model[1]['performance']['accuracy']:.2f}%\")\n",
    "print(f\"   Use case: When highest classification accuracy is critical\")\n",
    "\n",
    "# Best F1-Score\n",
    "best_f1_model = max(results.items(), key=lambda x: x[1]['performance']['f1_score'])\n",
    "print(f\"\\nüéØ BEST F1-SCORE:\")\n",
    "print(f\"   Model: {best_f1_model[0]}\")\n",
    "print(f\"   F1-Score: {best_f1_model[1]['performance']['f1_score']:.2f}%\")\n",
    "print(f\"   Use case: When balanced precision-recall is needed\")\n",
    "\n",
    "# Fastest inference\n",
    "best_speed_model = min(results.items(), key=lambda x: x[1]['inference_time']['avg_per_image_ms'])\n",
    "print(f\"\\n‚ö° FASTEST INFERENCE:\")\n",
    "print(f\"   Model: {best_speed_model[0]}\")\n",
    "print(f\"   Speed: {best_speed_model[1]['inference_time']['avg_per_image_ms']:.2f} ms/image\")\n",
    "print(f\"   Throughput: {best_speed_model[1]['inference_time']['throughput_img_per_s']:.2f} images/sec\")\n",
    "print(f\"   Use case: Real-time applications, edge devices\")\n",
    "\n",
    "# Most efficient (smallest parameters)\n",
    "best_size_model = min(results.items(), key=lambda x: x[1]['parameters']['total'])\n",
    "print(f\"\\nüíæ SMALLEST MODEL:\")\n",
    "print(f\"   Model: {best_size_model[0]}\")\n",
    "print(f\"   Parameters: {best_size_model[1]['parameters']['total']/1e6:.2f}M\")\n",
    "print(f\"   Size: {best_size_model[1]['parameters']['size_mb']:.2f} MB\")\n",
    "print(f\"   Use case: Memory-constrained environments, mobile deployment\")\n",
    "\n",
    "# Best trade-off (accuracy vs speed)\n",
    "print(f\"\\n‚öñÔ∏è BEST TRADE-OFF (Accuracy vs Speed):\")\n",
    "tradeoff_scores = {}\n",
    "for model_name, result in results.items():\n",
    "    acc_score = result['performance']['accuracy'] / 100\n",
    "    speed_score = 1 / result['inference_time']['avg_per_image_ms']\n",
    "    tradeoff_scores[model_name] = 0.6 * acc_score + 0.4 * speed_score\n",
    "\n",
    "best_tradeoff = max(tradeoff_scores.items(), key=lambda x: x[1])\n",
    "print(f\"   Model: {best_tradeoff[0]}\")\n",
    "print(f\"   Accuracy: {results[best_tradeoff[0]]['performance']['accuracy']:.2f}%\")\n",
    "print(f\"   Inference: {results[best_tradeoff[0]]['inference_time']['avg_per_image_ms']:.2f} ms/image\")\n",
    "print(f\"   Use case: Production applications requiring both accuracy and efficiency\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CELL 16: SUMMARY & CONCLUSION\n",
    "# ======================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ EXPERIMENT COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - best_ViT_Tiny.pth\")\n",
    "print(\"  - best_DeiT_Tiny.pth\")\n",
    "print(\"  - best_Swin_Tiny.pth\")\n",
    "print(\"\\nAll visualizations displayed above.\")\n",
    "print(\"\\nHardware used:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  - GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"  - CPU: {platform.processor()}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nModels Trained:\")\n",
    "for model_name in results.keys():\n",
    "    print(f\"  ‚úì {model_name}\")\n",
    "print(f\"\\nTotal Models: {len(results)}\")\n",
    "print(f\"Dataset Classes: {num_classes}\")\n",
    "print(f\"Device Used: {device.upper()}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (VIT GPU)",
   "language": "python",
   "name": "vit-comparison"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
